# DeepNN-MNIST-Project

## Overview
This project implements neural network-based classification models from scratch and using popular deep learning frameworks to classify handwritten digits from the MNIST dataset. The assignment explores different activation functions, optimization algorithms, and regularization techniques to improve model performance.

Additionally, binary classification tasks on synthetic linearly and non-linearly separable datasets are tackled using both logistic regression and deep neural networks, demonstrating the superiority of deep models for complex classification problems.

## Features
- Feed-forward neural network implementation from scratch with ReLU and Softmax activations.
- Experiments with different activation functions: Sigmoid, Tanh, ReLU, LeakyReLU.
- Comparison of optimization algorithms: Stochastic Gradient Descent (SGD) and Adam.
- Regularization methods including weight decay (L2), dropout, and early stopping.
- Logistic regression with IRLS for linear classification.
- Deep neural network classification for binary linearly and non-linearly separable data.
- Visualization of activation potentials in 3D space to understand learned representations.


